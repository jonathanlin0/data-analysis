I'm trying to learn more about data analysis methods, including artificial intelligence. This repo contains the code related to that.

I am also keeping a private document containing everything I'm learning that I may publicly release in the future.

*Note to self:* I deleted the local 145 GB ImageNet dataset to free up some disk space on my pc. At the time of deleting these files, I forgot how this repo works, so it's probably best to ask a coding agent (cursor, claude code, etc.) to sift through the repo (and read the readme) to figure out how to import the ImageNet data, setup the ImageNet data, and get all the models working, if you want to play around with this repo again. The bottom of this README mentions a util script that may be related to setting up imagenet. At the time of writing this, the format was `/data/datasets/ImageNet/{train|val <- "train" or "val" as the folder name}/{some random id}/{another random id}.JPEG`

#### Implemented Projects
- Spam text classifier. Implemented with RNN and LSTM
- Hand written number generator. Implemented using GANs
- Image upscaler. Input a low resolution image, and the model outputs a higher resolution of the same image. Implemented using GANs

## RNN

### `single_layer_rnn.py`
This is a single layer rnn that tries to predict the next number. The dataset is the sin function. This isn't great cause there's no noise in the data, so the RNN can overfit as much as it wants, and it's simply interpreted as being "good." 

---

### `multi_layer_rnn.py`
Made a more complex RNN. Has multiple layers and an embedding layer to embed text to an intermediate n-dimensional vector. Its task is to classify whether a given text is spam or not. It didn't take a high value of `embed_size`, `hidden_size`, or number of layers to achieve the maximum score of 86.64%. This proves that unmodified, vanilla RNNs don't scale well to compute.

Conclusion: The upper capabilities of RNNs are reached easily.

## LSTM

### `lstm.py`
Implemented an LSTM with 2 fc connected layers at thje end. Didn't achieve any results better than the RNN implementation. Tried a few different parameters. Reached the same maximum accuracy of 86.64%.

## GAN
An annoying thing to note about training GANs is that the "power" of the generator and discriminator have to be approximately equal, and their loss has to decrease approximately at the same rate. If one is significantly better than the other, than results won't be ideal. For example, if the generator is a lot more "powerful" than the discriminator, then the generator can figure out a specific set of outputs that will always trick the discriminator, even if that manipulation isn't related to the type of images the generator is actually trying to generate.

Note: the context above is about GANs used for generating images. But it's applicable to any general GAN's task of trying to replicate a probability distribution.

### `gan_mnist.py`
This GAN generates hand written numbers using the MNIST dataset. The following image shows hand written numbers generated by the GAN. The GAN worked pretty well, since we can tell what number most of the generated samples are. The GAN did not work that well to more complex and rich images.

![GAN generated MNIST](figs/GAN_mnist.png)

This project could be extended to generate handwritten text.

#### Method 1
Create a GAN for each possible character. Then use the generator of each GAN network to create the individual letters for whatever you want to write.

#### Method 2
Create a GAN network that's trained on all the characters. Then get a really good character classifier (discriminator) to choose which specific character you want to type.

### `gan_upscale.py`
This GAN upscales the resolution of images. It takes in a 3 x 64 x 64 image and outputs a 3 x 224 x 224 image. The random input noise that usually is fed into a generator in a GAN is instead a low resolution image. Then the output is the high resolution image.

The dataset was created with originally high resolution images. Then, I used Pytorch's resizing function to resize them to smaller dimensions, thereby creating lower resolution images that can map to high resolution images.

The GAN did a decent job at upscaling, but there's still a noticable difference between the high resolution image generated by the GAN and the original high resolution images. This makes sense, as there's information loss in a lower resolution photo that isn't deterministic when trying to be upscaled. The GAN generated image looks like if you were looking at the original high resolution image with blurry glasses on. But I think this performance is bad, because I find that I would even have trouble determining details of a blurry image. Below are figures for the GAN's performance. In each group of horizontal triple images, the very left is the low resolution image, the middle is the high resolution image generated by the GAN, and the right is the original high resolution image.

### Final Results:
![GAN upscaling final result](figs/GAN_upscaling_final.png)
### Initial Results:
![GAN upscaling final result](figs/GAN_upscaling_initial.png)

Next steps: compare the GAN's results to other algorithmic methods.

## Utility Files

### `util/unzip_imagenet_train.bat`
The imagenet training data is really weird. It's a zipped folder of zipped folders. So this script unzips all the "mini" folders, assuming that the parent `.tar` folder has already been unzipped. So this script extracts the child `.tar` folders that contain the actual photos.
